/*
初始化加载插件
*/
def initPlugin(){
    plugins = select * from files(getHomeDir().split("/clusterDemo/")[0]+"/plugins") where isDir=true
    ex = ["user",NULL]
    try{
        if(!(`mqtt in plugins[`filename])){
            installPlugin("mqtt")
        }
        if(!(`kafka in plugins[`filename])){
            installPlugin("kafka")
        }
        cntMqtt = exec count(*) from defs() where name like "mqtt::%"
        if(cntMqtt == 0){
            loadPlugin("mqtt")
        }   
        cntKafka = exec count(*) from defs() where name like "kafka::%"
        if(cntKafka == 0){
            loadPlugin("kafka")
        }  
    }catch(ex){}
    if(ex[1]!=NULL){
        throw(ex[1])
    }
}
initPlugin();
go;

/*
增加连接
参数：args 为标准json，需要包含以下字段：
commonParams:
name: 连接名称     字符串
protocol: 协议类型 字符串
host: 主机地址     字符串
port: 端口         整数
mqttParams：
username: 用户名   字符串
password: 密码     字符串
使用示例：
args = '{ "name":"emqx", "protocol":"mqtt", "host":"183.134.101.140", "port":1883, "username":"admin1", "password":"DolphinDB123"}' 
dcp_addConnect(args) 
args = '{ "name":"kafka1", "protocol":"kafka", "host":"183.134.101.140", "port":9092}'  
dcp_addConnect(args)
*/

def dcp_addConnect(args){
    connectArgs = parseExpr(args).eval()
    ex=["user",NULL]
    try{
        if(strlen(string(connectArgs[`name]))>50){
            throw("The connection name cannot be longer than 50 characters")
        }
        exits = select * from loadTable("dfs://dataAcquisition","connectInfo") where name = connectArgs[`name]
        if(exits.size()!=0){
            throw("The connection name already exists, please verify the connection information")
        }else{
                id = rand(uuid(),1)[0]
                if(connectArgs[`protocol]=="kafka"){
                    connectArgs[`username]=string(NULL)
                    connectArgs[`password]=string(NULL)    
                }
                connectTable = table(id as id,connectArgs[`name] as name ,connectArgs[`protocol] as protocol,
                connectArgs[`host] as host,connectArgs[`port] as port,connectArgs[`username] as username,connectArgs[`password] as password)
                update connectTable set createTime = now()
                update connectTable set updateTime = now()
                connectTable.reorderColumns!(loadTable("dfs://dataAcquisition","connectInfo").columnNames())
                loadTable("dfs://dataAcquisition","connectInfo").append!(connectTable)
                writeLog("The "+ upper(connectArgs[`protocol]) +" connection named " + connectArgs[`name]+" successfully added")
                return '{"status":1,"id":"'+string(id)+'"}'
        }
    }catch(ex){
        writeLog("An error occured when adding "+upper(connectArgs[`protocol])+" connection, the reason is:"+ex[1])
    }
    if(ex[1]!=NULL){
        throw(ex[1])
    }
}




/*
修改连接
参数：args 为标准json，可包含以下字段：
commonParams：
id(必选): 连接ID    字符串
name: 连接名称      字符串
host: 主机地址      字符串
port: 端口         整数
mqttParams:
username: 用户名    字符串
password: 密码      字符串
使用示例：
args = '{ "name":"emqx", "id":"cc2a890b-bd79-f243-da4c-7604bc9e4970", "host":"183.134.101.141", "port":1883, "username":"admin1", "password":"DolphinDB123"}'
dcp_updateConnect(args)  
args = '{ "name":"KAFKA", "id":"e5d6a0ed-600d-d701-3fcb-8d046883d517", "host":"183.134.101.142", "port":9092}'  
dcp_updateConnect(args)
*/
def dcp_updateConnect(args){
    connectArgs = parseExpr(args).eval()
    ex=["user",NULL]
    tmp = select * from loadTable("dfs://dataAcquisition","connectInfo") where id = uuid(connectArgs[`id])
    exitsSub = select * from loadTable("dfs://dataAcquisition","subscribeInfo") where connectId	= uuid(connectArgs[`id]) and subId is not null  
    try{
        if(strlen(string(connectArgs[`name]))>50){
            throw("The connection name cannot be longer than 50 characters")
        }
        if(exitsSub.size() != 0){
            throw("There are started subscription under the connection "+tmp[`name][0]+", the connection cannot be modified.")
        } 
        exits = select * from loadTable("dfs://dataAcquisition","connectInfo") 
        where name = connectArgs[`name] and protocol = tmp[`protocol][0] and id != uuid(connectArgs[`id])    
        if(exits.size()!=0){
            throw("The connection name already exists, please verify the connection information")
        }
        for(col in connectArgs.keys()){
            if(col !=`id){ 
                tmp[col]=connectArgs[col]
            }
        }
        tmp[`updateTime]=now()  
        delete from loadTable("dfs://dataAcquisition","connectInfo") where id = uuid(connectArgs[`id])
        loadTable("dfs://dataAcquisition","connectInfo").append!(tmp)
        writeLog("The "+ upper(tmp[`protocol][0]) +" connection named " + connectArgs[`name]+" successfully updated")
        return '{"status":1}'
    }catch(ex){
        writeLog("An error occured when updating "+upper(tmp[`protocol][0])+" connection, the reason is:"+ex[1])
    }
    if(ex[1]!=NULL){
        throw(ex[1])
    }
}


/*
删除连接,删除连接时会同时清理该连接下的所有订阅及订阅信息
参数：args 为标准json，包含以下字段：
ids: 连接ID  string数组
使用示例：
args = '{"ids":["8a6a92fe-7622-8a1d-446d-00af0c835906","45d4aa2a-5bab-a4d0-27a2-c8131c9f5a0a"],"dropUseTable":true}'
dcp_deleteConnect(args)
*/
def dcp_deleteConnect(args){
    connectArgs = parseExpr(args).eval()
    ex=["user",NULL]
    connectInfo =select * from loadTable("dfs://dataAcquisition","connectInfo") where id in uuid(connectArgs[`ids])
    subIds = select connectId,subId,name as subName,subNode,templateParams,handlerId from  loadTable("dfs://dataAcquisition","subscribeInfo") where connectId in uuid(connectArgs[`ids])
    tmp = select * from connectInfo left join subIds on connectInfo.id=subIds.connectId  
    mqttInfo = select * from tmp where protocol = `mqtt
    kafkaInfo  = select * from tmp where protocol = `kafka
    try{ 
        exitsSubConnectNames = exec name from tmp group by name having(count(subId)>0) 
        if(exitsSubConnectNames.size() != 0){
            throw("There are started subscription under the connection ["+concat(exitsSubConnectNames,",")+"], the connection cannot be deleted.")
        } 
        if(connectArgs[`dropUseTable] and subIds.size()!=0){    
            templateParams = subIds.templateParams
            subNodes = subIds.subNode
            for(i in (0..(templateParams.size()-1))){//i = 0
                t = parseJsonTable(templateParams[i])
                tbName = t[0].value
                node = subNodes[i]
                scripts = "
                use ops;
                tbName = `"+tbName+";   
                unsubscribeAll(tbName);
                if(existsStreamTable(tbName)){
                    dropStreamTable(tbName)
                }"
                rpc(node,runScript,scripts)
            }
        }
        delete from loadTable("dfs://dataAcquisition","subscribeInfo") where connectId in uuid(connectArgs[`ids])   
        delete from loadTable("dfs://dataAcquisition","connectInfo") where id in uuid(connectArgs[`ids])
        handerIds = subIds.handlerId    
        for(handlerId in handerIds){
            update loadTable("dfs://dataAcquisition","ParserTemplate") set citeNumber = citeNumber-1 where id=uuid(handlerId)
        }
        if(mqttInfo.size()!=0){
            writeLog("The MQTT connection named [" + concat(distinct(mqttInfo[`name]),",")+"] successfully deleted")
        }
        if(kafkaInfo.size()!=0){
            writeLog("The KAFKA connection named [" + concat(distinct(kafkaInfo[`name]),",")+"] successfully deleted")
        }
    }catch(ex){
        writeLog("An error occured when deleting  connection , the reason is:"+ex[1])
    }
    if(ex[1]!=NULL){
        throw(ex[1])
    }
}

/*
获取连接列表  
@args 无
@return 连接列表json,{"mqtt": [], "kafka": []}
*/
def dcp_getConnectList(){
    connectAll = select * from loadTable("dfs://dataAcquisition","connectInfo") order by updateTime desc
    protocolList = exec distinct protocol from connectAll
    res = '{'
    for(index in 0..(protocolList.size()-1)){
        tmp = select * from connectAll where protocol = protocolList[index]
        res +='"'+protocolList[index]+'":'+toStdJson(tmp)
        if(index == protocolList.size()-1){
            res +='}'        
        }else{
            res +=','        
        }
    }
    return res
}


//返回kafka消費者參數列表
def dcp_getKafkaConsumerCfgList(){
    consumerCfgList = '["builtin.features","client.id","message.max.bytes","message.copy.max.bytes",
    "receive.message.max.bytes","max.in.flight.requests.per.connection","max.in.flight","topic.metadata.refresh.interval.ms","metadata.max.age.ms",
    "topic.metadata.refresh.fast.interval.ms","topic.metadata.refresh.fast.cnt","topic.metadata.refresh.sparse","topic.metadata.propagation.max.ms",
    "topic.blacklist","debug","socket.timeout.ms","socket.blocking.max.ms","socket.send.buffer.bytes","socket.receive.buffer.bytes","socket.keepalive.enable",
    "socket.nagle.disable","socket.max.fails","broker.address.ttl","broker.address.family","socket.connection.setup.timeout.ms","connections.max.idle.ms",
    "reconnect.backoff.jitter.ms","reconnect.backoff.ms","reconnect.backoff.max.ms","statistics.interval.ms","enabled_events","error_cb","throttle_cb",
    "stats_cb","log_cb","log_level","log.queue","log.thread.name","enable.random.seed","log.connection.close","background_event_cb","socket_cb","connect_cb",
    "closesocket_cb","open_cb","resolve_cb","opaque","default_topic_conf","internal.termination.signal","api.version.request","api.version.request.timeout.ms",
    "api.version.fallback.ms","broker.version.fallback","allow.auto.create.topics","security.protocol","ssl.cipher.suites","ssl.curves.list","ssl.sigalgs.list",
    "ssl.key.location","ssl.key.password","ssl.key.pem","ssl_key","ssl.certificate.location","ssl.certificate.pem","ssl_certificate","ssl.ca.location",
    "ssl.ca.pem","ssl_ca","ssl.ca.certificate.stores","ssl.crl.location","ssl.keystore.location","ssl.keystore.password","ssl.providers","ssl.engine.location",
    "ssl.engine.id","ssl_engine_callback_data","enable.ssl.certificate.verification","ssl.endpoint.identification.algorithm","ssl.certificate.verify_cb",
    "sasl.mechanisms","sasl.mechanism","sasl.kerberos.service.name","sasl.kerberos.principal","sasl.kerberos.kinit.cmd","sasl.kerberos.keytab",
    "sasl.kerberos.min.time.before.relogin","sasl.username","sasl.password","sasl.oauthbearer.config","enable.sasl.oauthbearer.unsecure.jwt",
    "oauthbearer_token_refresh_cb","sasl.oauthbearer.method","sasl.oauthbearer.client.id","sasl.oauthbearer.client.secret","sasl.oauthbearer.scope",
    "sasl.oauthbearer.extensions","sasl.oauthbearer.token.endpoint.url","plugin.library.paths","interceptors","group.id","group.instance.id",
    "partition.assignment.strategy","session.timeout.ms","heartbeat.interval.ms","group.protocol.type","group.protocol","group.remote.assignor",
    "coordinator.query.interval.ms","max.poll.interval.ms","enable.auto.commit","auto.commit.interval.ms","enable.auto.offset.store","queued.min.messages",
    "queued.max.messages.kbytes","fetch.wait.max.ms","fetch.queue.backoff.ms","fetch.message.max.bytes","max.partition.fetch.bytes","fetch.max.bytes",
    "fetch.min.bytes","fetch.error.backoff.ms","offset.store.method","isolation.level","consume_cb","rebalance_cb","offset_commit_cb","enable.partition.eof",
    "check.crcs","client.rack","transactional.id","transaction.timeout.ms","enable.idempotence","enable.gapless.guarantee","queue.buffering.max.messages",
    "queue.buffering.max.kbytes","queue.buffering.max.ms","linger.ms","message.send.max.retries","retries","retry.backoff.ms","retry.backoff.max.ms",
    "queue.buffering.backpressure.threshold","compression.codec","compression.type","batch.num.messages","batch.size","delivery.report.only.error","dr_cb",
    "dr_msg_cb","sticky.partitioning.linger.ms","client.dns.lookup","request.required.acks","acks","request.timeout.ms","message.timeout.ms",
    "delivery.timeout.ms","queuing.strategy","produce.offset.report","partitioner","partitioner_cb","msg_order_cmp","opaque","compression.codec",
    "compression.type","compression.level","auto.commit.enable","enable.auto.commit","auto.commit.interval.ms","auto.offset.reset","offset.store.path",
    "offset.store.sync.interval.ms","offset.store.method","consume.callback.max.messages"]'
    return '{"consumerCfgList":' + consumerCfgList + '}'    
}

/*
获取可用节点列表（数据节点+计算节点），
参数：args 为标准json，需要包含以下字段：
protocol： 协议类型 字符串
使用示例：
args = '{"protocol":"mqtt"}'    
dcp_getNodes(args)
*/
def dcp_getNodes(args){
    nodes = exec name from rpc(getControllerAlias(),getClusterPerf) where mode in [0,3,4] and state=1
    res = array(STRING)
    protocol=parseExpr(args).eval()[`protocol]
    existsFunc = def(protocol){
        func =protocol+"::%"
        return (exec count(*) from defs() where name like func)>0
    }
    for(node in nodes){
        if(rpc(node,existsFunc{protocol})){
            res.append!(node)
        }
    }
    return '{"nodes":'+toStdJson(res)+'}'
}


/*
添加订阅信息：
参数：args 为标准json，需要包含以下字段：
commonParams:
name: 订阅名称                    字符串
topic: 订阅主题                   字符串
connectId：使用的连接             字符串
handlerId：使用的解析模板         字符串（parseJson为false时不需要该参数） 
templateParams: 模板参数          字符串
parseJson: 是否解析json数据        bool  
mqttParams:
recvbufSize：接收缓冲区大小       整数（默认20480）
kafkaParams:
partition: 分区                    整数（默认为NULL）
offset: 偏移量                     整数（默认为NULL）
consumerCfg：kafka消费配置         字符串   
使用示例：
args = '{"name":"test","topic":"topic1","parseJson":false,
"templateParams":\'[{\"key\":\"outputTableName\",\"value\":\"dcp_test\"}]\',"connectId":"f88c0d8b-7f53-0f64-3cc3-e79ff6568154"}'
dcp_addSubscribe(args)
args = '{"name":"test","topic":"topic1","parseJson":false,
"templateParams":\'[{\"key\":\"outputTableName\",\"value\":\"dcp_test1\"}]\',
"consumerCfg":\'[{\"key\":\"group.id\",\"value\":\"test\"}]\',
"connectId":"ecffc67c-e6d2-4a72-63cf-386e097016bc"}'
dcp_addSubscribe(args)
*/
def dcp_addSubscribe(args){
    subArgs = parseExpr(args).eval()
    connectInfo = exec protocol,name from loadTable("dfs://dataAcquisition","connectInfo") where id = uuid(subArgs['connectId'])
    proto = connectInfo[`protocol][0]
    connectName = connectInfo[`name][0]
    subNode = iif(subArgs['subNode']==string(NULL),getNodeAlias(),subArgs['subNode'])
    ex = ["user",NULL]
    try{
        if(strlen(string(subArgs[`name]))>50){
            throw("The subscribe name cannot be longer than 50 characters") 
        }
        cnt = exec count(*) from loadTable("dfs://dataAcquisition","subscribeInfo") where name = subArgs['name'] and connectId=uuid(subArgs['connectId'])
        if(cnt !=0){
            throw("The "+ upper(proto)+" subscribe name already exists in connection " +connectName+" , please verify the subscribe information")
        }
        id  = rand(uuid(),1)
        subInfo = table(id as id,[uuid(subArgs['connectId'])] as connectId)
        update subInfo set topic = subArgs['topic']
        update subInfo set name = subArgs['name']
        update subInfo set createTime = now()
        update subInfo set updateTime = now()
        update subInfo set subId=string(NULL)
        update subInfo set status=0
        update subInfo set templateParams=subArgs['templateParams']
        update subInfo set parseJson=subArgs['parseJson']
        update subInfo set subNode = subNode
        if(proto=="mqtt"){
            update subInfo set recvbufSize = iif(subArgs['recvbufSize']==string(NULL),20480,subArgs['recvbufSize'])
            update subInfo set consumerCfg = string(NULL)
            update subInfo set partition = int(NULL)
            update subInfo set offset = int(NULL)
        }else if(proto=="kafka"){
            update subInfo set recvbufSize = int(NULL)
            update subInfo set consumerCfg = iif(subArgs['consumerCfg']==string(NULL),string(NULL),subArgs['consumerCfg'])  
            update subInfo set partition = iif(subArgs['partition']==int(NULL),int(NULL),subArgs['partition'])   
            update subInfo set offset = iif(subArgs['offset']==int(NULL),int(NULL),subArgs['offset'])   
        }else{
            throw("the protocol: "+proto + " is not supported ")    
        }   
        if(subArgs[`parseJson]){
            update subInfo set handlerId = uuid(subArgs['handlerId'])
            update loadTable("dfs://dataAcquisition","ParserTemplate") set citeNumber = citeNumber+1 where id = uuid(subArgs['handlerId'])   
        }
        else{
            handlerId = (exec id from loadTable("dfs://dataAcquisition","ParserTemplate") where flag=1 and protocol =  proto )[0]
            update subInfo set handlerId = handlerId
            update loadTable("dfs://dataAcquisition","ParserTemplate") set citeNumber = citeNumber+1 where id = handlerId   
        }
        subInfo.reorderColumns!(loadTable("dfs://dataAcquisition","subscribeInfo").columnNames())
        loadTable("dfs://dataAcquisition","subscribeInfo").append!(subInfo)
        logInfo ="The "+ upper(proto) +" connection named " + connectName+" successfully added subscribe "+subArgs['name']
        rpc(subNode,writeLog{logInfo})
        return '{"status":1,"id":"'+string(id)[0]+'"}'  
    }catch(ex){
        writeLog("An error occured when adding "+upper(proto)+" subscribe info, the reason is:"+ex[1])
    }
    if(ex[1]!=NULL){
        throw(ex[1])
    }
}


/*
修改订阅
参数：args 为标准json，需要包含以下字段：
commonParam：
id: 订阅ID                  字符串
name：订阅名称               字符串    
topic: 订阅主题              字符串
subNode:订阅节点             字符串
handlerId：使用的解析模板     字符串
templateParams: 模板参数     修改handlerId,必须同时有templateParams参数
parseJson: 是否解析json数据   bool  
mqttParam:
recvbufSize: 接收缓冲区大小   整数   
kafkaParam:
partition: 分区              整数
offset: 偏移量                整数
consumerCfg: kafka消费配置    字符串    
内部调用：
status：是否是启动更新         整数
使用示例:
args = '{"name":"test","topic":"topic1","handlerId":1,"parseJson":false, 
"templateParams":\'[{\"key\":\"outputTableName\",\"value\":\"dcp_test\"}]\',"id":"e7183216-9f11-4d12-b227-4477f9f20313"}'
dcp_updateSubscribe(args)
*/
args = '{"id":"123456","subId":"123456"}'
def dcp_updateSubscribe(args){
    subArgs = parseExpr(args).eval()
    subscribeInfo = select * from loadTable("dfs://dataAcquisition","subscribeInfo") where id=uuid(subArgs[`id])
    connectInfo = exec * from loadTable("dfs://dataAcquisition","connectInfo") where id=uuid(subscribeInfo[`connectId][0])
    proto,connectName,connectIdNow = connectInfo[`protocol][0],connectInfo[`name][0],connectInfo[`id][0]
    subNode = subscribeInfo[`subNode][0]
    ex = ["user",NULL]
    try{
        if(strlen(string(subArgs[`name]))>50){
            throw("The subscribe name cannot be longer than 50 characters") 
        }
        cnt = exec count(*) from loadTable("dfs://dataAcquisition","subscribeInfo") where name = subArgs['name'] and connectId=uuid(connectIdNow)
        and id != uuid(subArgs['id'])
        if(cnt !=0){
            throw("The "+ upper(proto)+" subscribe name already exists in connection "+connectName+", please verify the subscribe information")
        }
        for(col in subArgs.keys()){
            if(col != "id"){
                if(col !="handlerId"){
                    subscribeInfo[col] = subArgs[col]  
                }else{
                    subscribeInfo[col] = uuid(subArgs[col])
                }
            }
        }
        if(subscribeInfo[`handlerId][0]!=uuid(subArgs[`handlerId]) and subArgs[`handlerId]!=NULL){
            update loadTable("dfs://dataAcquisition","ParserTemplate") set citeNumber = citeNumber+1 where id = uuid(subArgs[`handlerId])
            update loadTable("dfs://dataAcquisition","ParserTemplate") set citeNumber = citeNumber-1 where id = subscribeInfo[`handlerId]   
        }
        if(!subArgs[`parseJson]){
            handlerId = (exec id from loadTable("dfs://dataAcquisition","ParserTemplate") where flag=1 and protocol =  proto)[0]
            update subscribeInfo set handlerId = handlerId
        }
        subscribeInfo[`updateTime] = now()  
        subscribeInfo.reorderColumns!(loadTable("dfs://dataAcquisition","subscribeInfo").columnNames())  
        delete from loadTable("dfs://dataAcquisition","subscribeInfo") where id=uuid(subArgs[`id])  
        loadTable("dfs://dataAcquisition","subscribeInfo").append!(subscribeInfo)
        if(subArgs[`status]==int(NULL)){
            logInfo = "the subscribe "+subArgs['name']+" of "+ upper(proto) +" connection named " + connectName+" successfully updated"
            rpc(subNode,writeLog{logInfo})
        }
        return '{"status":1}'
    }catch(ex){
        writeLog("An error occured when updating "+upper(proto)+" subscribe info, the reason is:"+ex[1])
    }
    if(ex[1]!=NULL){
        throw(ex[1])
    }
}

/*
删除订阅
参数：args 为标准json，需要包含以下字段：
ids: 订阅ID int数组
dropUseTable:删除订阅时是否删除流表 bool
使用示例：
args = '{"ids":["aef4faac-f6d7-6c4e-d122-c96d7e3ebab1","eb07cafb-4e15-e795-5ffc-422177abe8c9"],"dropUseTable":true}' 
dcp_deleteSubscribe(args)
*/
def dcp_deleteSubscribe(args){
    subArgs = parseExpr(args).eval()
    subInfo = select * from loadTable("dfs://dataAcquisition","subscribeInfo") where id in uuid(subArgs[`ids])
    connectInfo = select protocol,name from loadTable("dfs://dataAcquisition","connectInfo") where id in subInfo[`connectId]
    proto,connectName = connectInfo[`protocol][0],connectInfo[`name][0]
    ex=["user",NULL]
    try{
        if(subArgs[`dropUseTable]){
            templateParams = subInfo.templateParams
            subNodes = subInfo.subNode
            for(i in (0..(templateParams.size()-1))){//i = 0
                t = parseJsonTable(templateParams[i])
                tbName = t[0].value
                node = subNodes[i]
                scripts = "
                use ops;
                tbName = `"+tbName+";   
                unsubscribeAll(tbName);
                if(existsStreamTable(tbName)){
                    dropStreamTable(tbName)
                }"
                rpc(node,runScript,scripts)
            }
        }
        delete from loadTable("dfs://dataAcquisition","subscribeInfo") where id in uuid(subArgs[`ids])
        citeNumber = select count(*)  from subInfo group by handlerId
        for(i in citeNumber){
            update loadTable("dfs://dataAcquisition","ParserTemplate") set citeNumber = citeNumber - i.count where id = uuid(i.handlerId)
        }
        writeLog("the subscribe ["+concat(subInfo.name,",")+"] of "+ upper(proto) +" connection named " + connectName+" successfully deleted")
    }catch(ex){
        writeLog("An error occured when deleting "+upper(proto)+" subscribe info, the reason is:"+ex[1])
    }
    if(ex[1]!=NULL){
        throw(ex[1])
    }
}

/*
查看订阅状态
@args subId:订阅ID 字符串
@return 返回一个包含订阅状态的json，键为subStatus   
*/
def dcp_getSubStatus(args){
    subArgs = parseExpr(args).eval()
    subs =select id,subId,connectId from loadTable("dfs://dataAcquisition","subscribeInfo") where id=uuid(subArgs[`subId])
    connInfo = select id,protocol from loadTable("dfs://dataAcquisition","connectInfo") where id = uuid(subs.connectId[0])    
    if(connInfo.protocol == "mqtt"){
        tmp = select * from mqtt::getSubscriberStat() where subscriptionId = subs.subId
    }else if(connInfo.protocol=="kafka"){
        tmp = select * from kafka::getJobStat() where subscriptionId = subs.subId
    }
    return '{"subStatus":'+toStdJson(tmp)+'}'
}


/*
检测连接下的订阅状态
将库中状态为开启状态但实际已经不存在的订阅关闭
@args connectId:连接ID UUID
args='{"connectId":"8c50939f-51c6-382f-b501-c4a495c240ca"}'
*/
def dcp_checkSubStatus(args){
    subArgs = parseExpr(args).eval()
    subs =select id,subNode,subId,connectId from loadTable("dfs://dataAcquisition","subscribeInfo") where connectId=uuid(subArgs[`connectId])
    connInfo = select id,protocol from loadTable("dfs://dataAcquisition","connectInfo") where id = uuid(subArgs[`connectId])    
    subNodes = distinct(subs.subNode)
    if(subNodes.size()==0){
        return '{"status":1}'
    }
    if(connInfo.protocol == "mqtt"){
        res = each(rpc{,mqtt::getSubscriberStat},subNodes)
        for(i in (0..(subNodes.size()-1))){
            update res[i] set node = subNodes[i] 
        }
        nowSub = res.unionAll()
    }else if(connInfo.protocol=="kafka"){
        res = each(rpc{,kafka::getJobStat},subNodes)
        for(i in (0..(subNodes.size()-1))){
            update res[i] set node = subNodes[i] 
        }
        nowSub = res.unionAll()
    }
    tmp = select * from subs left join nowSub on subs.subId = nowSub.subscriptionId and subs.subNode = nowSub.node
    id = tmp.id 
    subId = tmp.subId  
    user = tmp.user 
    if(subId.size() > 0 ){
        for(i in (0..(subId.size()-1))){  //i=0
            if(user[i]==NULL and subId[i]!=NULL){ 
                argsNew='{"id":"'+string(id[i])+'","subId":NULL,"status":0}' 
                dcp_updateSubscribe(argsNew) 
            }
        }
    }
    return '{"status":1}'
}


/*
获取订阅列表
参数：args 为标准json，需要包含以下字段：
connectId: 连接ID   UUID
*/
def dcp_getConnectAndSubInfo(args){
    try{dcp_checkSubStatus(args)}catch(ex){throw ex[1]}
    subArgs = parseExpr(args).eval()
    connect = select * from loadTable("dfs://dataAcquisition","connectInfo") where id = uuid(subArgs['connectId']) order by updateTime desc
    subscribe = select * from loadTable("dfs://dataAcquisition","subscribeInfo") where connectId = uuid(subArgs['connectId']) order by updateTime desc
    return '{"connectInfo":'+toStdJson(connect[0])+',"subscribes":'+toStdJson(subscribe)+',"total":'+subscribe.size()+'}'      
}


/*
开始订阅
参数：args 为标准json，需要包含以下字段：
subId: 订阅ID   
args = '{"subId":"729127e1-32f5-fc1f-8fb2-e2c80986ec94"}'
*/
def dcp_startSubscribe(args){
    subArgs = parseExpr(args).eval()
    //通过订阅id获取链接信息，解析模板信息
    sub = select * from loadTable("dfs://dataAcquisition","subscribeInfo")  where id = uuid(subArgs[`subId])
    connInfo = select * from loadTable("dfs://dataAcquisition","connectInfo")  where id in sub.connectId
    tmp =  (exec * from sub left join connInfo  on connInfo.id=sub.connectId)[0]
    node = sub[`subNode][0]
    ex=['user',NULL]
    try{
        if(!sub.parseJson){
            tableName = string(parseJsonTable(sub['templateParams'][0]).value[0])
            if(!rpc(node,existsStreamTable{tableName})){
                streamTableScripts = "enableTableShareAndPersistence(streamTable(1:0,[`topic,`msg,`ts],[STRING,STRING,TIMESTAMP]),string(`"+tableName+"),,,100000,,,100000)"
                rpc(node,runScript,streamTableScripts)
                logInfo ="successfuly initialized the default streamTable "+tableName+" for subscribe "+sub.name[0]+" of "+upper(connInfo.protocol[0])+" connection "+connInfo.name[0] 
                rpc(node,writeLog{logInfo})
            }
        }else{
            tableName = string(parseJsonTable(sub['templateParams'][0]).value[0])
            if(!rpc(node,existsStreamTable{tableName})){
                throw("The table "+tableName+" doesn't exist in the node "+ node+"!") 
            }
        }
        //初始化解析函数
        handler = (exec handler from loadTable("dfs://dataAcquisition","ParserTemplate") where id = uuid(tmp[`handlerId]))[0]
        if(handler==NULL){
            throw("The handler of subscribe "+sub.name+" doesn't exist!")   
        }
        rpc(node,runScript,handler)
        //获取解析函数名
        handlerName = handler.split("\n")
        handlerName  = (handlerName[strpos(handlerName,"def")!=-1].split("(")[0].split("def ")[1])[0]
        //拼接订阅脚本，在定义函数时，因为handlerName是动态的，所以无法直接在订阅时指定函数名并执行，需要拼接成字符串后，在调用时通过runScript延迟执行   
        subscribeStr = 
        'sub = select * from loadTable("dfs://dataAcquisition","subscribeInfo")  where id=uuid("'+ subArgs[`subId]+'");
        connInfo = select * from loadTable("dfs://dataAcquisition","connectInfo")  where id in sub.connectId;
        tmp =  (exec * from sub left join connInfo  on connInfo.id=sub.connectId)[0];'
        if(tmp.protocol == "mqtt"){
            //拼接mqtt
            subscribeStr +=  'mqtt::subscribe(tmp["host"],tmp["port"],tmp["topic"],,'+handlerName+'{'
            mqsubArgs = parseExpr(tmp.templateParams).eval() 
            for(i in (0..(mqsubArgs.size()-1))){    
                if(i < mqsubArgs.size()-1 ){
                    subscribeStr += "`"+mqsubArgs[i][`value]+","
                }
                else{
                    subscribeStr += "`"+mqsubArgs[i][`value]
                }
            }
            subscribeStr += '},tmp["username"],tmp["password"],tmp["recvbufSize"]);'
        }
        else if(tmp.protocol == "kafka"){
            //初始化消费者配置
            subscribeStr +='consumerCfg = dict(string, string);
            consumerCfg["metadata.broker.list"] = tmp.host+":"+tmp.port;
            cfgTable = parseJsonTable(tmp.consumerCfg,table([`key,`value] as name,[`STRING,`STRING] as type));
            for(i in cfgTable){
                consumerCfg[i.key]=i.value
            }
            consumer = kafka::consumer(consumerCfg);
            topic,partition,offset=[tmp.topic],[tmp.partition],[tmp.offset]; 
            //根据参数选择订阅方式，订阅主题还是分区
            if(partition[0]==int(NULL)){
                kafka::subscribe(consumer, topic);
            }else{
                kafka::assign(consumer, topic, partition, offset)  
            };'
            kasubArgs = parseExpr(tmp.templateParams).eval()
            //第一个解析模板参数必须为表明，根据是否有其他参数，拼接subscribe函数   
            if(kasubArgs.size()==1){
                subscribeStr += 'kafka::createSubJob(consumer,'+kasubArgs[0].value+','+handlerName+',"'+tmp.topic+'");' 
            }else{
                subscribeStr += 'kafka::createSubJob(consumer,'+kasubArgs[0].value+','+handlerName+'{'  
                for(i in (1..(kasubArgs.size()-1))){    
                    if(i < kasubArgs.size()-1 ){
                        subscribeStr += "`"+kasubArgs[i][`value]+","
                    }
                    else{
                        subscribeStr += "`"+kasubArgs[i][`value]
                    }
                }
                subscribeStr += '},"'+tmp.topic+'");'      
            } 
        }   
        rpc(node,runScript,subscribeStr)    
        if(tmp.protocol == "mqtt"){
            //获取mqtt订阅ID
            subscriptionId=exec last(subscriptionId) from rpc(node,mqtt::getSubscriberStat) order by createTimestamp
        }else if(tmp.protocol == "kafka"){
            subscriptionId=exec last(subscriptionId) from rpc(node,kafka::getJobStat)  order by createTimestamp
        }   
        logInfo = "the subscribe "+tmp.name+" of "+upper(tmp.protocol)+" connection "+tmp.connInfo_name+" successfully started"
        rpc(node,writeLog{logInfo})
        update loadTable("dfs://dataAcquisition","ParserTemplate") set useNumber = useNumber + 1 where id = uuid(tmp[`handlerId])
        argsNew='{"id":"'+subArgs[`subId]+'","subId":"'+subscriptionId+'","status":1}' 
        dcp_updateSubscribe(argsNew)
    }catch(ex){
        writeLog("An error occured when starting subscribe "+tmp.name+" of "+upper(tmp.protocol)+" connection "+tmp.connInfo_name+" , the reason is:"+ex[1])
    }
    if(ex[1]!=NULL){
        throw(ex[1])
    }
}


/*
停止订阅
参数：args 为标准json，需要包含以下字段：
subId: 订阅ID  string数组 
使用示例：
args = '{"subId":["e7183216-9f11-4d12-b227-4477f9f20313","508dd3c4-1e1d-69c2-2bb2-05a1a2f26c6d"]}'  
dcp_stopSubscribe(args)
args='{"subId":["dff94d45-ca64-ec2a-197f-247950f972a4","01948441-ade6-00b7-5eaf-4b029adb6909"]}'
*/
def dcp_stopSubscribe(args){
    subArgs = parseExpr(args).eval()
    subId = select id,subNode,subId,handlerId,connectId,name from loadTable("dfs://dataAcquisition","subscribeInfo") where id in uuid(subArgs[`subId])
    connInfo = select id,protocol,name from loadTable("dfs://dataAcquisition","connectInfo") where id in subId.connectId  
    info = select * from subId left join connInfo on connInfo.id=subId.connectId 
    ex=['user',NULL] 
    try{  
        for(i in (0..(subId.size()-1))){  //i=1
            if(info[i].subId != string(NULL)){
                if(info[i].protocol == "mqtt"){
                    rpc(info[i].subNode,mqtt::unsubscribe{info[i].subId})
                }else if(info[i].protocol == "kafka"){
                    rpc(info[i].subNode,kafka::cancelSubJob{info[i].subId})
                }
                logInfo = "The subscribe "+info[i].name+" of "+upper(info[i].protocol)+" connection "+info[i].connInfo_name+" successfully stoped"
                rpc(info[i].subNode,writeLog{logInfo})
                update loadTable("dfs://dataAcquisition","ParserTemplate") set useNumber = useNumber - 1 where id = uuid(info[i].handlerId)
                argsNew='{"id":"'+string(info[i].id)+'","subId":NULL,"status":0}' 
                dcp_updateSubscribe(argsNew) 
            }   
        }
    }catch(ex){
        writeLog("An error occured when stoping subscribe of "+upper(info[i].protocol)+" connection "+info[i].connInfo_name+" , the reason is:"+ex[1])
    }
    if(ex[1]!=NULL){
        throw(ex[1])
    }
    
}


/*
解析模板参数
@args 
handler:解析函数的函数体 string
protocol:解析函数对应的协议 string
@return 返回一个参数列表（拼接为字符串）
*/
def dcp_parseTemplateArgs(handler,protocol){
    if(protocol==`mqtt){
        handlerParam = handler.split("){")[0].split("(")[1].split(",")
        handlerParam = handlerParam[0:(handlerParam.size()-2)]    
    }else if(protocol==`kafka){
        handlerParam = handler.split("){")[0].split("(")[1].split(",")
        handlerParam = [`outTableName] <- handlerParam[0:(handlerParam.size()-3)] 
    }
    paramList = '['
    for(index in 0..(handlerParam.size()-1)){   
        if(index < (handlerParam.size()-1)){    
            paramList += '"'+handlerParam[index]+'",'  
        }else{
            paramList += '"'+handlerParam[index]+'"]'
        }
    }
    return paramList   
}

/*
添加模板
@args 
name:模板名称 字符串
handler:函数定义 字符串
protocol:模板协议 字符串
comment:模板备注 字符串
@return 无
*/
def dcp_addHandler(args){
    handlerArgs = parseExpr(args).eval() 
    ex=['user',NULL]
    try{
        if(strlen(string(handlerArgs[`name]))>50){
            throw("The parserTemplate name cannot be longer than 50 characters")    
        }
        cnt = exec count(*) from loadTable("dfs://dataAcquisition","ParserTemplate") where name = handlerArgs['name'] and protocol=handlerArgs[`protocol]
        if(cnt !=0){
            throw("The "+ upper(handlerArgs[`protocol])+" parserTemplate name "+handlerArgs[`name]+" already exists, please verify the template information")
        }
        id = rand(uuid(),1)
        templateParams = dcp_parseTemplateArgs(handlerArgs[`handler],handlerArgs['protocol'])
        flag = iif(handlerArgs["flag"]==NULL,0,1)
        tmp = table(id as id, [handlerArgs['name']] as name, [handlerArgs[`handler]] as handler, [handlerArgs['protocol']] as `protocol,
        string([handlerArgs['comment']]) as comment,[flag] as flag,[templateParams] as templateParams,[0] as citeNumber,[0] as useNumber,
        [now()] as createTime,[now()] as updateTime)  
        loadTable("dfs://dataAcquisition","ParserTemplate").append!(tmp)
        writeLog("The "+upper(handlerArgs[`protocol])+ " handler "+handlerArgs[`name]+" successfully added")
        return '{"status":1,"id":"'+string(id[0])+'"}'   
    }catch(ex){
        writeLog("An error occured when adding The "+upper(handlerArgs[`protocol])+ " handler "+handlerArgs[`name]+", the reason is:"+ex[1])
    } 
    if(ex[1]!=NULL){
        throw(ex[1])
    }
}

/*
修改模板
@args 
必选：
id:模板id 整数
可选：
name:模板名称 字符串
protocol:协议 字符串
handler:函数定义 字符串
comment:模板备注 字符串
@return 无
*/
def dcp_updateHandler(args){  
    handlerArgs = parseExpr(args).eval()    
    ex=['user',NULL]
    try{
        if(strlen(string(handlerArgs[`name]))>50){
            throw("The parserTemplate name cannot be longer than 50 characters")    
        }
        exits = select * from loadTable("dfs://dataAcquisition","ParserTemplate") 
        where name = handlerArgs[`name] and protocol = handlerArgs[`protocol] and id != uuid(handlerArgs[`id])    
        if(exits.size()!=0){
            throw("The "+ upper(handlerArgs[`protocol])+" parserTemplate name "+handlerArgs[`name]+" already exists, please verify the template information")
        }
        ParserTemplate = select * from loadTable("dfs://dataAcquisition","ParserTemplate") where id=uuid(handlerArgs[`id])
        update ParserTemplate set name = handlerArgs[`name] 
        update ParserTemplate set protocol = handlerArgs[`protocol] 
        update ParserTemplate set handler = handlerArgs[`handler] 
        update ParserTemplate set comment = handlerArgs[`comment] 
        update ParserTemplate set updateTime = now() 
        delete from loadTable("dfs://dataAcquisition","ParserTemplate") where id=uuid(handlerArgs[`id])
        loadTable("dfs://dataAcquisition","ParserTemplate").append!(ParserTemplate)
        writeLog("The "+upper(handlerArgs[`protocol])+ " handler "+handlerArgs[`name]+" successfully updated")
        return '{"status":1}'   //更新成功  
    }catch(ex){
        writeLog("An error occured when updating The "+upper(handlerArgs[`protocol])+ " handler "+handlerArgs[`name]+", the reason is:"+ex[1])
    }
    if(ex[1]!=NULL){
        throw(ex[1])
    }
}    


/*
删除链接
@args 
ids:模板id string数组
args = '{"ids":[]}'
*/
def dcp_deleteHandler(args){
    handlerArgs = parseExpr(args).eval()
    handlerInfo = select * from loadTable("dfs://dataAcquisition","ParserTemplate") where id in uuid(handlerArgs[`ids])
    mqttInfo = select * from handlerInfo where protocol=`mqtt
    kafkaInfo = select * from handlerInfo where protocol=`kafka
    ex=['user',NULL]
    try{
        delete from loadTable("dfs://dataAcquisition","ParserTemplate") where id in uuid(handlerArgs[`ids])
        if(mqttInfo.size()!=0){
            writeLog("The MQTT handler ["+concat(mqttInfo[`name],",")+"] successfully deleted")
        }
        if(kafkaInfo.size()!=0){
            writeLog("The KAFKA handler ["+concat(kafkaInfo[`name],",")+"] successfully deleted")
        }
        return '{"status":1}'   //删除成功  
    }catch(ex){
        writeLog("An error occured when deleting the handler "+handlerArgs[`name]+", the reason is:"+ex[1])
    }
    if(ex[1]!=NULL){
        throw(ex[1])
    }
}



/*
获取点位解析模板列表
@args "protocol":解析模板协议 字符串(默认为空)
@return 返回一个包含解析模板信息的json，键为items，值为解析模板列表 
*/  
def dcp_getParserTemplateList(args="{}"){
    protocolType=parseExpr(args).eval()['protocol']
    if(protocolType==string(NULL)){
        tmp = select * from loadTable("dfs://dataAcquisition","ParserTemplate") order by updateTime desc    
    }else{
        tmp = select * from loadTable("dfs://dataAcquisition","ParserTemplate") where protocol=protocolType order by updateTime desc
    } 
    return '{"items":'+toStdJson(tmp)+',"total":'+tmp.size()+'}'  
}


/*
获取模板参数
@args handlerId:模板ID int整数
@return 返回一个包含模板参数的json，键为returnArgs，值为参数列表   
*/
def dcp_getTemplateArgs(args){
    handlerArgs = parseExpr(args).eval()
    handler = (exec templateParams from loadTable("dfs://dataAcquisition","ParserTemplate") where id = uuid(handlerArgs['handlerId']))[0]
    return '{"returnArgs":'+handler+'}'    
}


//初始化解析模板
def initParserTemplate(){   
    //初始化解析模板
    handler = '
    def msgHandler(outputTableName,timeColumn,idColumn,topic,message){
        tempdict = parseExpr(message).eval()
        keys = tempdict.keys()
        if(timeColumn==NULL){
            ts = now()
        }else{
            ts = temporalParse(tempdict[timeColumn],"yyyy-MM-dd HH:mm:ss")
        }
        id = tempdict[idColumn] 
        for(i in keys){
            if(!(i in [timeColumn,idColumn] )){
                tmp = table([ts] as ts,[id] as id,[i] as metric,[tempdict[i]] as value)
                objByName(outputTableName).append!(tmp)            
            }
        }
    }
    '
    args = '{"name":"parseMqtt","handler":\''+handler+'\',"protocol":"mqtt","comment":"测试解析模板"}'   
    dcp_addHandler(args)


    handler = "
    def saveJsonMsg(outputTableName,topic,msg){
        objByName(outputTableName).append!(table([topic]as topic,[msg] as msg,[now()] as ts))
    }
    "
    args = '{"name":"saveJsonMsg","handler":\''+handler+'\',"protocol":"mqtt","comment":"转存json数据","flag":0}'   
    dcp_addHandler(args)

    handler = "
    def saveMsg(msg,key,topic){
        return table([topic] as topic,[msg] as dataContent,[now(true)] as startTime)    
    }
    "
    args = '{"name":"saveJsonMsg_kafka","handler":\''+handler+'\',"protocol":"kafka","comment":"转存json数据","flag":0}'   
    dcp_addHandler(args)


    handler='
     def parseKafka(timeColumn,idColumn,msg,key,topic){
        tempdict = parseExpr(msg).eval()
        writeLog("KAFKA DATA:"+msg)
        keys = tempdict.keys()
        rowNum = keys.size()-2
        tmp = table(rowNum:rowNum,[`ts,`id,`metric,`value],[TIMESTAMP,STRING,STRING,DOUBLE])    
        if(timeColumn==NULL or tempdict[timeColumn]==NULL){ 
            ts = now()
        }else{
            ts = temporalParse(tempdict[timeColumn],"yyyy.MM.ddTHH:mm:ss.SSS")
        }
        tmp[`ts] = ts
        tmp[`id] = tempdict[idColumn] 
        for(i in keys){
            if(!(i in [timeColumn,idColumn] )){
                tmp[`metric] =  take(i,rowNum) 
                tmp[`value] =   tempdict[i]
            }
        }
        return tmp  
    }'
    args = '{"name":"parseKafka","handler":\''+handler+'\',"protocol":"kafka","comment":"解析kafka json数据"}'   
    dcp_addHandler(args)

}

/*
获取调试日志
@args 标准json，需要包含以下字段：
protocol 协议 字符串
@return 协议相关日志
*/
def dcp_getLog(args){
    protocol = upper(parseExpr(args).eval()[`protocol])
    rowBytes = 5000000
    if(getNodeType()==3 or getNodeType()==2){
        //日志过短则offset为0   
        offset = iif(getServerLogLength(getNodeAlias())-rowBytes>0,getServerLogLength(getNodeAlias())-rowBytes,0)   
        log = getServerLog(rowBytes,offset,true,getNodeAlias()) as log
        res = log[regexFind(log,protocol)!=-1]
    }else if(getNodeType()==0 or getNodeType()==4){
        nodeInfo = select * from rpc(getControllerAlias(),getClusterPerf) where name = getNodeAlias()
        agentHost = nodeInfo[`agentSite].split(":")[0][0]  
        agentPort = nodeInfo[`agentSite].split(":")[1][0]  
        conn_agent = xdb(host=agentHost,port=int(agentPort))
        serverLogLength = remoteRun(conn_agent,"getServerLogLength",getNodeAlias())
        offset = iif(serverLogLength-rowBytes>0,serverLogLength-rowBytes,0)   
        log  = remoteRun(conn_agent,"getServerLog",rowBytes,offset,true,getNodeAlias()) as log
        res = log[regexFind(log,protocol)!=-1]  
        close(conn_agent)
    }else{
        ex = "the node type is not supported."
        throw(ex)
    }
    return reverse(res)
}


/*
初始化检测函数
@args null
@return 返回一个json，键为status，值为0或1，表示是否初始化完成  
*/
def dcp_exitsDataAcquisition(){
    cnt = exec count(*) from getFunctionViews() where name = "dcp_initFlag"
    if(existsDatabase("dfs://dataAcquisition") and cnt==1){
        return '{"status":1,"message":"初始化已完成"}'
    }else{
        return '{"status":0,"message":"还未初始化库表及函数视图"}'
    }   
}


def dcp_createAllTable(){
    db = database("dfs://dataAcquisition",VALUE,[`1])
    colNames = `id`name`protocol`host`port`username`password`createTime`updateTime
    colTypes = [UUID,STRING,STRING,STRING,INT,STRING,STRING,TIMESTAMP,TIMESTAMP]
    createTable(db,table(1:0,colNames,colTypes),`connectInfo)

    colNames = `id`connectId`subNode`topic`partition`offset`name`recvbufSize`handlerId`templateParams`consumerCfg`parseJson`status`subId`createTime`updateTime  
    colTypes = [UUID,UUID,STRING,STRING,INT,INT,STRING,INT,UUID,STRING,STRING,BOOL,INT,STRING,TIMESTAMP,TIMESTAMP]
    createTable(database("dfs://dataAcquisition"),table(1:0,colNames,colTypes),`subscribeInfo)  

    colNames = `id`name`handler`protocol`comment`flag`templateParams`citeNumber`useNumber`createTime`updateTime
    colTypes=[UUID,STRING,STRING,STRING,STRING,INT,STRING,INT,INT,TIMESTAMP,TIMESTAMP]
    createTable(database("dfs://dataAcquisition"),table(1:0,colNames,colTypes),`ParserTemplate)
}


def dcp_initFlag(){
    return 1
}

/*
初始化函数
*/
def dcp_init(){
    dcp_createAllTable()
    initParserTemplate()
    addFunctionView(dcp_initFlag)
}
